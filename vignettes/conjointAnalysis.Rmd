---
title: "Conjoint analysis: modeling preferences to calibrate vulnerability scoring"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Conjoint Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
  
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
# library(tidyverse)
# library(lubridate)
# library(readxl)
# 
# library(unhcrthemes)
# library(patchwork)
# 
# library(cregg)
# 
# knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
# knitr::opts_chunk$set(fig.height = 6, fig.width = 9, fig.retina = 1)
# 
# form <- read_excel(here::here("data-raw","form_final_withSpanish_radomized.xlsx"))
# data <- read_excel(here::here("data-raw","data.xlsx"))
# opts <- read_excel(here::here("data-raw","cja_opts_v4.xlsx"))
# 
# opts <- opts |> mutate(across(everything(), str_replace, "\r", ""))
# 
# dict <- 
#   opts |> 
#   distinct(dim, measure) |> 
#   arrange(dim, measure) |> 
#   with_groups(dim, mutate, feature = str_c("v", row_number()))
# 
# scores <- 
#   data |> 
#   select(email, starts_with("p.")) |> 
#   pivot_longer(-email, names_to = "var")
# 
# dims <- 
#   data |> 
#   select(starts_with("p.")) |> 
#   names() |> 
#   str_match("p\\.(.+)\\.(.+)") |> 
#   as_tibble(.name_repair = "universal") |> 
#   rename(var = ...1, dim = ...2, lvl = ...3) |> 
#   left_join(form |> select(lvl = name, label = `label::English (en)`), by = "lvl") |> 
#   separate_rows(label, sep = "\\n") |> 
#   with_groups(lvl, mutate, feature = str_c("v", row_number())) |> 
#   left_join(dict) |> select(-feature) |> 
#   pivot_wider(names_from = measure, values_from = label) |> 
#   mutate(across(-c(var:lvl), #as_factor))
#                 ~fct_relevel(., levels = opts |> filter(measure == cur_column()) |> pull(level))))
# 
# cjdata <- 
#   left_join(dims, scores, by = "var") |> 
#   nest(data = -dim) |> 
#   rowwise() |> 
#   mutate(data = data |> select(-where(compose(all, is.na))) |> list(),
#          formula = 
#            data |> 
#            select(where(is.factor)) |> 
#            names() |> 
#            str_c(collapse = "+") |> 
#            str_c("value", vars = _, sep = "~"),
#          margins = mm(data, as.formula(formula), id = ~email) |> list(),
#          amces = cj(data, as.formula(formula), id = ~email) |> list(),
#          importance = 
#            as_tibble(amces) |> 
#            mutate(estimate = abs(estimate)/sum(abs(estimate)),
#                   lower = abs(lower)/sum(abs(lower), na.rm = TRUE),
#                   upper = abs(upper)/sum(abs(upper), na.rm = TRUE)) |> list())
```



## What is Conjoint analysis?

Conjoint analysis can speed up expert consultations by offering an __objective mean to compile expert opinions__.

* Conjoint analysis originated in mathematical psychology by psychometricians.

* often used to evaluate how people make decisions between a set of different options when considering a number of criteria at the same time (conjoint features; “trade-offs”). 
 

## 1. Measurement framework

The [Joint Intersectoral Analysis Framework (JIAF)](https://interagencystandingcommittee.org/system/files/gbna_tools_and_guidance_pkg_final_june_2019.pdf) is a theoretical generic measurement framework to be used for Humanitarian needs assessment. It specifies three distinct and complementary components of humanitarian severity and vulnerability indexes:
  
* Basic Needs & Living standards
* Coping Capacity
* Well Being & Community integration

This generic model can be contextualised: different sub-indicators might be used for each of the 3 components depending on cultural and political situations. 
 
## 2. Define the combined alternatives to be compared 

* participants rate their preferences for profiles with different combinations of the attributes or criteria. 

* CA then allows to “decompose” or reverse-engineer these ratings into estimates of how important each criteria or attribute is to a participant’s ranking decisions
 
## 3. Utility scales & Agreement levels

Estimating the contribution of each potential answers

* Utility values indicate the overall contribution of each attribute to how the profiles were rated (e.g. whether number of meals is more important in vulnerability scoring than access to safe water). 

* A higher _"utility"_ estimate indicates that this level contributes to a higher vulnerability than the level with the lower utility estimate (it does not give an absolute value for the utility of an option, but rather assumes a reference alternative). 

* Standard deviation for each level within model allows to better understand how homogenous the group of experts is with respect to one level.  
 

## 4. Importance of each criteria 

* Importance of each criteria represent the average importance as estimed from all experts. 

* Importance values will then be used as the weights for each attribute inside each of our three dimensions. 

* Importance values sum to 100%.

```{r cleaning}
# ##This is where cleaning is reported, identifying cases for deletion. 
# 
# #FOR TIME: It is recommended that experts complete the survey during the live exercise. A good window of time is between 10-90 minutes. Cases that were completed faster or slower than this should be considered for deletion. 
# #This code generated the table of flagged entries. click on 'expert_validation' on the Environment to see the table. If you identify any entry flagged as suspect, but that you still want to keep, change the reference values 10 or 90. 
# 
# time_validation <- 
#   data |> 
#   transmute(email,
#             dur = as.duration(end-start) |> as.numeric("minutes"),
#             too_short = dur < 10,
#             too_long = dur > 90,
#             suspect = too_short | too_long)
# # View(time_validation)
# data <- data |> anti_join(time_validation |> filter(suspect), by = "email")
# 
# #FOR DUPLICATE ENTIRES: Cleaning for duplicates based on name of expert. 
# 
# expertdups <- data %>% semi_join(count(., email) %>% filter(n > 1)) #this identifies duplicates by name. Look and manually choose which to keep. 
# #Example manual cleaning. 
# #data <- data[ !(data$"_id" %in% c("12729577")), ] #1 staff had duplicate entries. Kept most recent and shortest time. _id is 12729577
# 
# #FOR OUTLIERS: Screen for data quality.
# #Spot repetitive responses (example: answered all options with '3'), responses with little variance (all answers between 6-7, standard deviation less than 2). Also looks for averages that are very different from the rest of the responses. For example, someone's average score is 2 when the rest of the scores are between 6-7. Analysis can be repeated with and without this case to test and compare how it influences the results. 
# #OR
# #spotting outliers using Mahalanobis distance. 
# 
# mah.data <- data %>% select(starts_with("p.")) %>% select(where(is.numeric))
# expert <- data %>% mutate(mah.dist = mah.data %>% mahalanobis(., center = colMeans(.), cov = cov(.), tol = 1e-21)) #just added this ", tol=1e-21" right after cov(.). After error message: 
# #"system is computationally singular: reciprocal condition number = 4.08252e-20 #Input `mah.dist` is `mah.data %>% mahalanobis(., center = colMeans(.), cov = cov(.))`.
# outliers <- expert %>% filter(mah.dist > qchisq(.95, df = ncol(mah.data))) #Cutoff point of probability, can be modified. .99 extreme outliers, .95, .90, etc. 
# data <- data %>% anti_join(outliers, by = "_id") #filters the identified out. 
# #PERU - no outliers detected. 
# 
# nexpert <- n_distinct(data$email)
# #names(expert)
```

---

##  Expert Review

```{r}
# plot1 <- 
#   data |> 
#   ggplot(aes(as.numeric(survey_mins))) + 
#   geom_histogram(color = "white", fill = "#0072BC") + 
#   scale_y_continuous(expand = expansion(0, 0)) +
#   labs(title = "Time to complete survey",
#        x = "minutes", y = "# of experts") +
#   theme_unhcr()
```

---

```{r}
# plot2 <- 
#   data |> 
#   summarize(mean = rowMeans(data |> select(starts_with("p.")))) |> 
#   ggplot(aes(mean)) + 
#   geom_histogram(colour = "white", fill = "#0072BC", binwidth = 1) +
#   scale_y_continuous(expand = expansion(0, 0)) +
#   labs(title = "Average rating",
#        x = "average score", y = "# of experts") +
#   theme_unhcr()
# 
# plot1 + plot2 + plot_annotation(caption = glue::glue("based on {nrow(data)} expert consultations"))
```


---

```{r output, results='asis'}
# cjplot <- function(.x) {
#   ggplot(.x, aes(estimate, level, xmin = lower, xmax = upper)) + 
#   geom_pointrange() +
#   scale_y_discrete(labels = \(x) str_wrap(x, 40)) +
#   facet_wrap(vars(feature), ncol = 1, scales = "free_y") +
#   labs(caption = glue::glue("The lines around the point represent the confidence interval",
#                             "(the shorter the line, the more experts are in agreement)",
#                             .sep = "\n"),
#        x = NULL, y = NULL) +
#   theme_unhcr() +
#   theme(plot.caption = element_text(hjust = 1))
# }
# 
# cjout <- function(dim, margins, amces, importance, ...) {
#   cat("---\n\n")
#   cat(glue::glue("{dim}\n\n"))
#   cat("---\n\n")
#   cat("## Marginal Means\n\n")
#   print(cjplot(margins) + labs(title = dim, subtitle = "Marginal Means"))
#   cat("\n\n")
#   cat("## Average Marginal Component Effects (AMCEs)\n\n")
#   print(cjplot(amces) + labs(title = dim, subtitle = "Average Marginal Component Effects (AMCEs)"))
#   cat("\n\n")
#   cat("## Importance Weights\n\n")
#   print(cjplot(importance) + 
#           scale_x_continuous(labels = scales::label_percent()) + 
#           labs(title = dim, subtitle = "Importance Weights"))
#   cat("\n\n")
# }
# 
# pwalk(cjdata, cjout)
```
