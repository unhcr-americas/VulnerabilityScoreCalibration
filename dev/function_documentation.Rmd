---
title: "Function documentation"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r development, include=FALSE}
library(testthat)
```

```{r development-load}
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
```

<!--
 For each fo the function you need, add FUSEN chunk through the Rstudio add-in " add {fusen} chunks"
 -->
# data

## Built-in Data

This includes the skeleton to build the 2 questionnaires in different languages

## Demo Data

The demo data includes the list of indicators/criteria in different langues

```{r}

readxl::read_excel( system.file("data-demo/indicator_criteria.xlsx", package = "VulnerabilityScoreCalibration"),
                            sheet = "indicator")  |>
  dplyr::filter( language == "English (en)") |>
  dplyr::select( dimension, indicator, indicator_hint)

```

The following dataset has been anonymized from one of the field implementation.

```{r}
kobodata <-  system.file("data-demo/quadra_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/quadra_form.xlsx", package = "VulnerabilityScoreCalibration")


```

# Quadratic Voting

## quadratic_prepare

```{r function-quadratic_prepare}
#' quadratic_prepare
#' 
#' This function aims at quickly building a quadratic voting questionnaire
#' 
#' The questionnaire comes with limitation as it can process not more than 5 groups
#' of maximum 5 indicators. Each indicators is associated to different levels that
#' will be then assessed through conjoint analysis for the weighting stage
#' 
#' After quadratic voting, it is expected that the facilitation of the result 
#' interpretation should allow to reduce the numbers of indicators to a maximum of
#' 12 indicators
#' 
#' @param indicator a dataframe with max 5 groups of 5 indicators
#' 
#' @return a questionnaire... 
#' 
#' @export
quadratic_prepare <- function(indicator){
    
}
```

```{r example-quadratic_prepare}

indicator <-  system.file("data-demo/indicator_criteria.xlsx", 
                          package = "VulnerabilityScoreCalibration")
#quadratic_prepare(indicator)
```

```{r tests-quadratic_prepare}
test_that("quadratic_prepare works", {
  expect_true(inherits(quadratic_prepare, "function")) 
})
```

## quadratic_review

```{r function-quadratic_review}
#' quadratic_review
#' 
#' Explore the results from a quadratic voting consultations. 
#'  1. What are the prioritized Topics?
#'  2. How dispersed participants votes are? 
#'  3. Who is expecting or pushing back... on what? 
#'  
#' @param kobodata path to data collected through kobotoolbox
#' @param koboform form used to collected through kobotoolbox quadratic survey
#' 
#' 
#' @importFrom ggridges geom_density_ridges
#' @importFrom unhcrthemes theme_unhcr
#' @importFrom readxl read_excel
#' @importFrom dplyr arrange select rename  
#'                group_by first
#'                left_join filter mutate desc
#' @importFrom forcats fct_reorder	
#' @importFrom tidyselect starts_with
#' @importFrom stats sd reorder
#' @import ggplot2
#' @return list with data and standard plots
#' 
#' @export
quadratic_review <- function(kobodata, koboform){
  
  ## load data
  data_ind <- readxl::read_excel( kobodata,
                            sheet = 1)
  data_positions <- readxl::read_excel(kobodata,
                       sheet = "positions")
  
  #names(data_positions)
  
  ## load info from the form
  positions <- readxl::read_excel(koboform,
                       sheet = "positions") |>
    # names(positions)
    dplyr::select(topic, name) |>
    dplyr::rename(groups.positions.pos_name = name)
  
  # choices <- readxl::read_excel(koboform,
  #                      sheet = "choices") |>
  #   ## Rename and use what ever label set is coming first 
  #   dplyr::rename(label = dplyr::first(tidyselect::starts_with("label")) )
  
  
  ## Check who did not reply
  #part1 <- part |>
  # dplyr::left_join(ind, by = c( "email" )) |>
  # dplyr::filter(is.na(start))
  
  ## Check matching
  data_ind2 <- data_ind
  
  data11_positions <- data_positions |>
    dplyr::left_join(positions) |>
    ## Add Participant info
    dplyr::left_join(data_ind2, by = c("_submission__uuid" = "_uuid")) |>
    
    dplyr::filter(!(is.na(groups.positions.votes)))
  
  data1_positions <- data11_positions |>
    dplyr::select(
      email,
      topic,
      groups.positions.pos_name,
      groups.positions.pos_text,
      groups.positions.votes,
      groups.positions.vote_cost
    ) |>
    dplyr::mutate(
      position = paste0(groups.positions.pos_text),
      groups.positions.votes = as.numeric(groups.positions.votes),
      groups.positions.vote_cost = as.numeric(groups.positions.vote_cost)
    ) |>
    dplyr::group_by(position,
             groups.positions.pos_name,
             groups.positions.pos_text) |>
    dplyr::summarise(
      cnt = dplyr::n(),
      vote = sum(groups.positions.votes),
      votemean = mean(groups.positions.votes),
      votesd = stats::sd(groups.positions.votes),
      votecost = sum (groups.positions.vote_cost)
    )  |>
    dplyr::arrange(dplyr::desc(vote))
  
  ## Reshape the data to merge votes
  df <- data_positions |>
    ## Add theme
    dplyr::left_join(positions) |>
    ## Add Participant info
    dplyr::left_join(data_ind2, by = c("_submission__uuid" = "_uuid")) |>
    dplyr::filter(!(is.na(groups.positions.votes))) |>
    dplyr::filter(!(is.na(email))) |>
    dplyr::select(
      email,
      topic,
      groups.positions.pos_name,
      groups.positions.pos_text,
      groups.positions.votes,
      groups.positions.vote_cost
    ) |>
    dplyr::mutate(
      position = paste0(groups.positions.pos_text),
      posit = paste0(topic, "-", groups.positions.pos_name),
      groups.positions.votes = as.numeric(groups.positions.votes),
      groups.positions.vote_cost = as.numeric(groups.positions.vote_cost)
    ) |>
    dplyr::mutate(
      groups.positions.pos_text = forcats::fct_reorder(.f = groups.positions.pos_text,
                                              .x = groups.positions.vote_cost,
                                              .fun = mean)
    ) |>
    dplyr::mutate(position = forcats::fct_reorder(.f = position,
                                         .x = groups.positions.vote_cost,
                                         .fun = mean)) |>
    dplyr::mutate(posit  = forcats::fct_reorder(.f = posit ,
                                       .x = groups.positions.vote_cost,
                                       .fun = mean))
  
  
  ## What are the prioritized Topics?
  p <- ggplot(data1_positions) +
    aes(x = stats::reorder(position, vote),
        fill = votesd,
        weight = vote) +
    geom_bar() +
    #  scale_fill_viridis_c(option = "inferno", direction = 1) +
    scale_fill_distiller(palette = "Oranges", direction = -1) +
    coord_flip() +
    unhcrthemes::theme_unhcr(font_size = 14)  +
    theme(
      panel.grid.major.x  = element_line(color = "#cbcbcb"),
      panel.grid.major.y  = element_blank(),
      panel.grid.minor = element_blank()
      # legend.position= "none"
    ) +
    labs(
      title = "Validation of indicators to be scored in the Eligibility Scorecard",
      subtitle = "In addition to the total number of votes, we can also see the level of agreement by color: the darker the vote, the more consensus...",
      x = "",
      y = "",
      caption = paste0(
        "Data collected through quadratic voting - # of participants ",
        nrow(data_ind |> dplyr::filter(!(is.na(   email   ))))
      )
    )
  
  ## How dispersed participants votes are?
  p2 <- ggplot(df,
               aes(x = groups.positions.votes,
                   y = position)) +
    ggridges::geom_density_ridges(fill = "#00AFBB",
                        rel_min_height = 0.01,
                        alpha = 0.7) +
    unhcrthemes::theme_unhcr(font_size = 14)  +
    theme(
      panel.grid.major.x  = element_line(color = "#cbcbcb"),
      panel.grid.major.y  = element_blank(),
      panel.grid.minor = element_blank()
    ) +
    labs(
      title = "Dispersion of votes on indicators to prioritize",
      x = "",
      y = "",
      caption = paste0(
        "Data collected through a quadratic voting survey. # of participants to the consultation is    ",
        nrow(data_ind |> dplyr::filter(!(is.na(  email  ))))
      )
    )
  
  ## Who is expecting (or pushing back...) on what?
  p3 <- ggplot(df) +
    aes(x = stats::reorder(posit, groups.positions.votes),
        y = groups.positions.votes) +
    geom_bar(stat = "identity" ,
                   fill = "#0072BC") +
    facet_wrap(~ email, nrow = 3) +
    coord_flip() +
    unhcrthemes::theme_unhcr(font_size = 12,
                             rel_small = 6/9,
                             rel_tiny = 6/9,
                             rel_large = 12/8)  +
    theme(
      panel.grid.major.x  = element_line(color = "#cbcbcb"),
      panel.grid.major.y  = element_line(color = "#e1e1e1"),
      panel.grid.minor = element_blank(),
      legend.position = "none",
      panel.spacing = unit(0.1, "lines"),
      strip.background = element_rect(
        color = "black",
        size = 1.5,
        linetype = "solid"
      )
    ) +
    labs(
      title = "Individual Prioritisation of indicators to score",
      x = "",
      y = "",
      caption = paste0(
        "Data collected through a quadratic voting survey. # of participants to the consultation is  ",
        nrow(data_ind |> dplyr::filter(!(is.na(  email))))
      )
    )
  
  #p3
  results <- list(df =df,
                  topic_prioritisation = p,
                  vote_dispersion = p2,
                  individual_prioritisation = p3)
  return(results)
    
}
```

```{r example-quadratic_review}

# kobodata <- here::here("", "quadra_data.xlsx")
# koboform <- here::here("", "survey_quadraticvoting_CBI_Indicators.xlsx")

kobodata <-  system.file("data-demo/quadra_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/quadra_form.xlsx", package = "VulnerabilityScoreCalibration")
 
## Run the process
result <- quadratic_review(kobodata, koboform)

## Review output
result[["topic_prioritisation"]]

result[["vote_dispersion"]]

result[["individual_prioritisation"]]

```

```{r tests-quadratic_review}
test_that("quadratic_review works", {
  expect_true(inherits(quadratic_review, "function")) 
})
```

## quadratic_report

```{r function-quadratic_report}
# usethis::use_rmarkdown_template(
#   template_name = "quadratic_report",
#   template_dir = NULL,
#   template_description = "Results from Quadratic Voting",
#   template_create_dir = TRUE
# )
#' Generate Report From Quadratic Voting 
#'  
#' @param folder folder within your project where to put the generated report. 
#'              Folder will be created if it does not exist
#' @param data name of the file with data collected through kobotoolbox should be xlsx
#' @param form name of the file with form used to collected through kobotoolbox
#'              quadratic survey (should be xlsx)
#' 
#' @importFrom unhcrdown pptx_slides
#' @importFrom dplyr filter select pull
#' @importFrom rmarkdown render
#' @importFrom here here
#' 
#' @return nothing the file for the report is generated
#' 
#' @export 
#'

quadratic_report <- function(data = "data.xlsx",
                             form = "form.xlsx",
                             folder = "data-raw") {
  
  ## Create the outfolder if it does not exist
  output_dir <- paste0(getwd(),"/",folder)
  if (!dir.exists(output_dir)) { stop("you did not provided the name of an existing folder within you project")}

  rmarkdown::render(
    system.file("rmarkdown/templates/quadratic_report/skeleton/skeleton.Rmd", package = "VulnerabilityScoreCalibration"),
    output_file = here::here(folder, paste0('quadratic_report-', data, '.pptx') ),
    params = list( folder=  folder,
                   data = data,
                  form= form)  )
}

```


```{r example-quadratic_report}

## Example -> the report will be saved in the same folder...
# quadratic_report(data = "data.xlsx",
#                  form = "form.xlsx",
#                  folder = "data-raw")
```




# Conjoint Analysis

## conjoint_prepare

```{r function-conjoint_prepare}
#' conjoint_prepare
#' 
#' Generate an xlsform used to perform consultation for conjoint analysis.
#' 
#' The options that will be compared are the different levels of the section 
#' of indicators that should have been filtered through the quadratic voting 
#' stage.
#' 
#' One level can actually match multiple response options from the screening 
#' questionnaire 
#' 
#' @param opts a dataframe containing the options to compare
#' @param language what language to use in case of multiple language
#' @param form_title  Actividad #2: Calificación de perfiles de vulnerabilidad 
#' @param id_string vulnerability_rating
#' @param outdir   directory within project where to put the file
#' @param outfile path of the file...
#' 
#' @return a file in xlsform format
#' 
#' @importFrom readr read_csv
#' @importFrom writexl write_xlsx
#' @importFrom dplyr group_by group_map tibble slice_head slice_tail
#' @importFrom conjoint caFactorialDesign
#' @importFrom tidyr expand_grid
#' @importFrom purrr map imap pmap_dfr detect_index
#' @importFrom rlang list2 set_names
#' @export
conjoint_prepare <- function(opts,
                             language = "English (en)", 
                             form_title = "Conjoint Review", 
                             id_string = "conjoint_rev", 
                             outdir = "data-raw", 
                             outfile = "conjoint_form.xlsx"){

  ## is in package
#survey <- readr::read_csv("conjointSurvey.csv")
#choices <- read_csv("conjointChoices.csv")
  
## Build the profiles using a factorial design  
profiles <-  opts |>
  nest(data = -dim) |>
  (\(x) rlang::set_names(x$data, x$dim))() |>
  purrr::map( ~ dplyr::group_map( dplyr::group_by(., measure),
                   ~ rlang::list2(!!.y$measure := .x$level)) |>
         purrr::flatten() |>
         (\(x) tidyr::expand_grid(!!!x))()) |>
  purrr::map( ~ conjoint::caFactorialDesign(., type = "fractional")) |>
  purrr::imap( ~ purrr::pmap_dfr(
    .,
    ~ dplyr::tibble(
      type = "select_one scale",
      appearance = "horizontal-compact",
      required = TRUE,
      label = str_c(..., sep = "\n")
    )
  ) |>
    dplyr::mutate(name = str_c(.y, row_number(), sep = "_"), .before = 1)) |>
  purrr::imap( ~ bind_rows(
    list(type = "begin_group", name = .y, label = .y),
    .x,
    list(type = "end_group")
  )) |>
  dplyr::bind_rows()

## build the survey part
survey <-  dplyr::bind_rows(
    survey |> 
      dplyr::slice_head(n = purrr::detect_index(survey$name,
                                  ~ replace_na(. == "p", FALSE))),
    profiles,
    survey |> dplyr::slice_tail(n = -purrr::detect_index(survey$name, 
                                           ~ replace_na(. == "p", FALSE)))
  )

## Define the settings
settings <-  dplyr::tibble(form_title = form_title,
         id_string = id_string,
         style = "theme-grid")

writexl::write_xlsx(
  list(
    survey = survey,
    choices = choices,
    settings = settings
  ),
  path = here::here(outdir, oufile),
  format_headers = FALSE
)
    
}
```

```{r example-conjoint_prepare}


# indicator <-  system.file("data-demo/indicator_criteria.xlsx",
#                 package = "VulnerabilityScoreCalibration")
# opts <- read_excel("cja_opts_SAL.xlsx")
# 
# conjoint_prepare( opts = opts, 
#                   language = "Spanish (es)",
#                   form_title = "Actividad #2: Calificación de perfiles de 
#                            vulnerabilidad - El Salvador, 23 de marzo, 2023",
#                   id_string = "vulnerability_rating",
#                   outdir = "", 
#                   outfile = "form.xlsx" )
```

```{r tests-conjoint_prepare}
test_that("conjoint_prepare works", {
  expect_true(inherits(conjoint_prepare, "function")) 
})
```

## conjoint_review

```{r function-conjoint_review}
#' conjoint_review
#' 
#'  What is Conjoint analysis?
#'  
#'  Conjoint analysis can speed up expert consultations by offering an 
#'  __objective mean to compile expert opinions__.
#'  
#'  * Conjoint analysis originated in mathematical psychology by psychometricians.
#'  
#'  * often used to evaluate how people make decisions between a set of 
#'  different options when considering a number of criteria at the same time 
#'  (conjoint features; “trade-offs”). 
#'  
#'  1. Measurement framework
#'  
#'  The Joint Intersectoral Analysis Framework (JIAF) is a theoretical generic 
#'  measurement framework to be used for Humanitarian 
#'  needs assessment. It specifies three distinct and complementary components
#'   of humanitarian severity and vulnerability indexes:
#'    
#'  * Basic Needs & Living standards  
#'  
#'  * Coping Capacity
#'  
#'  * Well Being & Community integration
#'  
#'  This generic model can be contextualized: different sub-indicators might be 
#'  used for each of the 3 components depending on cultural and political situations. 
#'  
#'  2. Define the combined alternatives to be compared 
#'  
#'  * participants rate their preferences for profiles with different combinations
#'   of the attributes or criteria. 
#'  
#'  * CA then allows to “decompose” or reverse-engineer these ratings into
#'   estimates of how important each criteria or attribute is to a participant’s ranking decisions
#'  
#'  3. Utility scales & Agreement levels
#'  
#'  Estimating the contribution of each potential answers
#'  
#'  * Utility values indicate the overall contribution of each attribute to how 
#'  the profiles were rated (e.g. whether number of meals is more important in 
#'  vulnerability scoring than access to safe water). 
#'  
#'  * A higher _"utility"_ estimate indicates that this level contributes to a
#'   higher vulnerability than the level with the lower utility estimate 
#'   (it does not give an absolute value for the utility of an option, but rather
#'    assumes a reference alternative). 
#'  
#'  * Standard deviation for each level within model allows to better understand 
#'  how homogeneous the group of experts is with respect to one level.  
#'  
#'  4. Importance of each criteria 
#'  
#'   * Importance of each criteria represent the average importance as estimated from all experts.
#'  
#'   * Importance values will then be used as the weights for each attribute inside each of our three dimensions. 
#'  
#'   * Importance values sum to 100%. 
#'   
#' @param kobodata path to data collected through kobotoolbox
#' @param koboform form used to collected through kobotoolbox quadratic survey
#' @param  duration_min used to filter down expert contribution that would have 
#'                       taken less than a certain number of minutes (default is 10)  
#' @param  duration_max used to filter down expert contribution that would have 
#'                       taken more than a certain number of minutes (default is 40)   
#'   
#' @import ggplot2
#' @import patchwork
#' @importFrom unhcrthemes theme_unhcr
#' @import survival
#' @import sandwich
#' @import conjoint
#' 
#' @importFrom readxl read_excel
#' @importFrom stringr str_c str_replace str_match
#' @importFrom purrr compose
#' @importFrom tidyr separate_rows nest pivot_longer  pivot_wider
#' @importFrom cregg cj mm
#' @importFrom stats mahalanobis cov as.formula pchisq
#' @importFrom forcats as_factor
#' @importFrom lubridate as.duration
#' @importFrom tidyselect starts_with
#' @importFrom  dplyr select mutate distinct transmute
#'                    with_groups  as_tibble rowwise
#'                   arrange row_number anti_join first
#'                   across everything where summarize
#' 
#' @return a series of plot
#' 
#' @export
conjoint_review <- function(kobodata, 
                            koboform,
                            duration_min = 10, 
                            duration_max = 40){

  
  ## load info from the form
  form <- readxl::read_excel(koboform,
                       sheet = "survey") |>
        ## Rename and use what ever label set is coming first 
        dplyr::rename(label = dplyr::first(tidyselect::starts_with("label")),
                        hint = dplyr::first(tidyselect::starts_with("hint")))
  
  ## Get the levels for all indicators
  opts <- readxl::read_excel(koboform,
                       sheet = "opts") |> 
          dplyr::mutate( dplyr::across(
            .cols = dplyr::everything(),
            ~ stringr::str_replace( ., "\r", "" )   ) )
     
  ## Get the list of indicators
  dict <- opts |>
    dplyr::distinct(dim, measure) |>
    dplyr::arrange(dim, measure) |>
    dplyr::with_groups(dim, 
                       mutate, 
                       feature = stringr::str_c("v", 
                                                dplyr::row_number()))
  

  ## load data
  data <- readxl::read_excel(kobodata,
                         sheet = 1)
  
  ## extracting the scores
  scores <- data |>
    dplyr::select(email, dplyr::starts_with("p.")) |>
    tidyr::pivot_longer(-email, names_to = "var")
  
  ## Recombine per dimension.. 
  dims <- data |>
    dplyr::select(starts_with("p.")) |>
    names() |>
    stringr::str_match("p\\.(.+)\\.(.+)") |>
    dplyr::as_tibble(.name_repair = "universal") |>
    dplyr::rename(var = ...1, dim = ...2, lvl = ...3) |>
    dplyr::left_join(form |> 
                       dplyr::select(lvl = name, label = `label`), 
                     by = "lvl") |>
    tidyr::separate_rows(label, sep = "\\n") |>
    dplyr::with_groups(lvl, mutate, 
                       feature = stringr::str_c("v", 
                                                dplyr::row_number())) |>
    dplyr::left_join(dict, by = dplyr::join_by(dim, feature)) |> 
    dplyr::select(-feature) |>
    tidyr::pivot_wider(names_from = measure, values_from = label) |>
    dplyr::mutate(dplyr::across(-c(var:lvl), forcats::as_factor)) 

    #uncommented as_factor
    #   ~factor(., levels = opts |> filter(measure == cur_column()) |> pull(level)))) 
    
    
    ## Data Quality Review
  plot1 <- data |>
    ggplot(aes(as.numeric(survey_mins))) +
    geom_histogram(color = "white", 
                   fill = "#0072BC",
                   bins = 10) +
    scale_y_continuous(expand = expansion(0, 0)) +
    labs(subtitle = "Times to complete the consultation",
         x = "(minutes)", y = "# of participants") +
    unhcrthemes::theme_unhcr(font_size = 14, grid = "y")
  
  plot2 <- data |>
    dplyr::reframe(mean = rowMeans(data |> 
                                       dplyr::select(starts_with("p.")))) |>
    ggplot(aes(mean)) +
    geom_histogram(colour = "white",
                   fill = "#0072BC",
                   binwidth = 1) +
    scale_y_continuous(expand = expansion(0, 0)) +
    labs(subtitle = "Average score",
         x = "# de participantes", y = "") +
    unhcrthemes::theme_unhcr(font_size = 14, grid = "y")
  
    #require(patchwork)
    p <- plot1 + plot2 + plot_annotation(title = "Before cleaning", caption = glue::glue("Based on {nrow(data)} consultations"))
  
  ## Data Cleaning 
  # Assessment that were completed too fast or too slow are suspect
  # and should be considered for deletion. 
   data_clean <-  data |>
    dplyr::mutate(
      dur = lubridate::as.duration(end - start) |> 
        as.numeric("minutes"),
      too_short = dur < duration_min,
      too_long = dur > duration_max,
      suspect = too_short | too_long,
      too_short2 = survey_mins < duration_min,
      too_long2 = survey_mins > duration_max,
      suspect_duration = too_short2 | too_long2   ) #|>
     #dplyr::filter(! suspect2)
   
  # Cleaning for duplicates based on name of expert.
  # if Duplicate Kept shortest time.
  # expert_duplication <-   data |>
  #                 dplyr::semi_join(count(., email) |>
  #                      dplyr::filter(n > 1))
    
   # data_id_duplication <-  data_clean |>
   #  # dplyr::select(email, survey_mins ) |>
   #    dplyr::group_by(email) |>
   #    #dplyr::summarize(across(everything(), min)) 
   #    dplyr::summarize(across(all_of(survey_mins) , min)) 
   # 
   #  data_clean <- data_clean |> 
   #    dplyr::anti_join(data_id_duplication , 
   #                     by = "email")
  
  #FOR TIME: It is recommended that experts complete the survey during the live exercise. 

  #This code generated the table of flagged entries. click on 'expert_validation'
  #on the Environment to see the table. If you identify any entry flagged as suspect,
  #but that you still want to keep, change the reference values 10 or 90. 
  # data.dur <-  data |>
  #   dplyr::transmute(
  #     dur = lubridate::as.duration(end - start) |> 
  #       as.numeric("minutes"),
  #     too_short = dur < duration_min,
  #     too_long = dur > duration_max,
  #     suspect = too_short | too_long,
  #     too_short2 = survey_mins < duration_min,
  #     too_long2 = survey_mins > duration_max,
  #     suspect2 = too_short2 | too_long2
  #   )
  # data_clean <- data |> 
  #     dplyr::anti_join(time_validation |> 
  #                        dplyr::filter(suspect2), 
  #                      by = "email")


   
  # Spotting outliers using Mahalanobis distanceto detect repetitive responses 
  # (example: answered all options with '3'), responses 
  # with little variance (all answers between 6-7, standard deviation less than 2). 
  # Also looks for averages that are very different from the rest of the responses.
  # For example, someone's average score is 2 when the rest of the scores are 
  # between 6-7. Analysis can be repeated with and without this case to test and 
  # compare how it influences the results. 
    
    ## EXTRACT SCORE ONLY
    data.score <-   data_clean |> 
      dplyr::select(starts_with("p.")) |> 
      dplyr::select(where(is.numeric)) 
   
    #average mahalanobis distances of each row from the whole data set.
    mahalnobisval <- stats::mahalanobis(data.score,
                                    colMeans(data.score),
                                    stats::cov(data.score),
                                 tol = 1e-21)
    #just added this ", tol=1e-21" right after cov(.). After error message:
    #"system is computationally singular: reciprocal condition number = 4.08252e-20 
   
    pvalue <- stats::pchisq(mahalnobisval, 
                                df=3,
                                lower.tail=FALSE)
    pchi <- stats::pchisq(.99,  df = 3 )
    
    ## Bind outlier check!
    data_clean <- cbind(data_clean, mahalnobisval, pvalue, pchi)  |>
      dplyr::mutate(outlier_mahalnobis = dplyr::if_else(pvalue < 0.001, TRUE, FALSE),
                    outlier_mahalnobis2 = dplyr::if_else(mahalnobisval > pchi, TRUE, FALSE))  
    
# 
#   data_clean |>
#     dplyr::select(email,survey_mins,  suspect, suspect_duration, mahalnobisval,outlier_mahalnobis, outlier_mahalnobis2 )


  
    
    
  
    ## Compile results!!
    cjdata <-  dplyr::left_join(dims, scores, by = "var") |>
          tidyr::nest(data = -dim) |>
          dplyr::rowwise() |>
          ## Compile all the calculations!!
          dplyr::mutate(
            data = data |> 
              select(- purrr::compose( purrr::compose(all, is.na))) |> 
              list(),
            formula =
              data |>
              dplyr::select(where(is.factor)) |>
              names() |>
              stringr::str_c(collapse = "+") |>
              stringr::str_c("value", 
                             vars = _, 
                             sep = "~"),
              margins = cregg::mm(data, stats::as.formula(formula), id = ~ email) |> 
              list(),
            amces =
              cregg::cj(data, 
                        stats::as.formula(formula),
                        id = ~ email) |>
              dplyr::group_by(feature) |>
              dplyr::mutate(
                normalized = (estimate - min(estimate)) / 
                                (max(estimate) - min(estimate)),
                horizontal = (estimate - min(estimate)),
                normalized1p = normalized + 1,
                horizontal1p = horizontal + 1 ) |> 
              dplyr::ungroup() |> 
              list(),
            importance =
              dplyr::as_tibble(amces) |>
              dplyr::mutate(
                estimate = abs(estimate) / sum(abs(estimate)),
                lower = abs(lower) / sum(abs(lower), na.rm = TRUE),
                upper = abs(upper) / sum(abs(upper), na.rm = TRUE)  ) |> 
              list()
          )


 

  results <- list(
    cjdata = cjdata,
    weights = df <- plyr::ldply(cjdata$amces, data.frame) |>
                dplyr::select(feature, level, normalized1p),
    data_quality = p
  )
   return(results)
}
```

```{r example-conjoint_review}

kobodata <-  system.file("data-demo/conjoint_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/conjoint_form.xlsx", package = "VulnerabilityScoreCalibration") 

cj <- conjoint_review(kobodata, koboform)

cj[["data_quality"]]
```

```{r tests-conjoint_review}
test_that("conjoint_review works", {
  expect_true(inherits(conjoint_review, "function")) 
})
```

## conjoint_plot_point 

```{r function-conjoint_plot_point}
#' conjoint_plot_point
#' 
#' Plot results per dimension - Marginal Means
#' 
#' @param .x A list or atomic vector.
#' @import ggplot2
#' @importFrom unhcrthemes theme_unhcr
#' @importFrom stringr str_wrap
#' 
#' @return a ggplot2 object
#' @export

conjoint_plot_point <- function(.x) {
    ggplot(.x,
           aes(estimate, 
               level, 
               xmin = lower, 
               xmax = upper)) +
      geom_pointrange() +
      scale_y_discrete(labels = \(x) stringr::str_wrap(x, 80)) +
      facet_wrap(vars(feature), ncol = 1, 
                 scales = "free_y") +
      labs(
        caption = glue::glue(
          "Las l\u00edneas alado de los puntos representan el intervalo de confianza.",
          "(Entre m\u00e1s corta la l\u00ednea, hubo m\u00e1s acuerdo entre participantes.)",
          .sep = "\n"
        ),
        x = NULL,
        y = NULL
      ) +
      unhcrthemes::theme_unhcr() +
      theme(plot.caption = element_text(hjust = 1))
  }
```

```{r example-conjoint_plot_point}

kobodata <-  system.file("data-demo/conjoint_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/conjoint_form.xlsx", package = "VulnerabilityScoreCalibration") 

cj <- conjoint_review(kobodata, koboform)
 
conjoint_plot_point( as.data.frame(cj[["cjdata"]][1,][["margins"]])) + 
  ggplot2::labs( subtitle = "Margins)")

conjoint_plot_point( as.data.frame(cj[["cjdata"]][1,][["amces"]])) + 
  ggplot2::labs( subtitle = "Average Marginal Component Effects (AMCEs)")

conjoint_plot_point( as.data.frame(cj[["cjdata"]][1,][["importance"]])) + 
  ggplot2::labs( subtitle = "Importance")
```

```{r tests-conjoint_plot_point}
test_that("conjoint_plot_point works", {
  expect_true(inherits(conjoint_plot_point, "function")) 
})


```

## conjoint_plot_bar - Average Marginal Component Effects (AMCEs)

```{r function-conjoint_plot_bar}
#' conjoint_plot_bar
#' 
#' Plot results per dimension - Average Marginal Component Effects (AMCEs)
#' 
#' @param .x A list or atomic vector.
#' @return gggplot2 graph
#' @import ggplot2
#' @importFrom unhcrthemes theme_unhcr
#' @importFrom stringr str_wrap
#' 
#' @return a ggplot2 object
#' 
#' @export
conjoint_plot_bar <- function(.x) {
    ggplot(.x, aes(horizontal,
                   level, 
                   label = round(horizontal, 2))) +
      geom_col(fill = "#0072BC") +
      geom_label() +
      scale_y_discrete(labels = \(x) stringr::str_wrap(x, 90)) +
      facet_wrap(vars(feature), ncol = 1, scales = "free_y") +
      unhcrthemes::theme_unhcr() +
      theme(plot.caption = element_text(hjust = 1))
  }
```

```{r example-conjoint_plot_bar}

kobodata <-  system.file("data-demo/conjoint_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/conjoint_form.xlsx", package = "VulnerabilityScoreCalibration") 

cj <- conjoint_review(kobodata, koboform)
 
## Plot AMCES as bar for dimension 2
conjoint_plot_bar( as.data.frame(cj[["cjdata"]][2,][["amces"]])) + 
  ggplot2::labs( subtitle = "Average Marginal Component Effects (AMCEs)")

## Plot importance as bar for dimension 2
conjoint_plot_bar( as.data.frame(cj[["cjdata"]][2,][["importance"]])) + 
  ggplot2::labs( subtitle = "Importance")
```

```{r tests-conjoint_plot_bar}
test_that("conjoint_plot_bar works", {
  expect_true(inherits(conjoint_plot_bar, "function")) 
})
```

## conjoint_walk - Summary by dimension

```{r function-conjoint_walk}
#' conjoint_walk 
#' 
#' Print out a summary from the conjoint analysis  
#' 
#'   *  Marginal Means
#' 
#'   *  Average Marginal Component Effects (AMCEs)
#' 
#'   *  Average Marginal Component Effects (AMCEs)
#' 
#'   *  Importance Weights
#' 
#' @param dim dimension of analysis
#' @param margins margins
#' @param amces amces 
#' @param importance importance
#' @param ... other argument
#' 
#' @importFrom glue glue
#' @import ggplot2
#' 
#' @return print some line for a notebook..
#' 
#' @export
conjoint_walk <- function(dim, margins, amces, importance, ...) {
    cat("---\n\n")
    cat(glue::glue("{dim}\n\n"))
    cat("---\n\n")
    cat("## Average Marginal Component Effects (AMCEs) - Bar \n\n")
    print(conjoint_plot_bar(amces) + ggplot2::labs(title = dim, subtitle = "Average Marginal Component Effects (AMCEs)"))
    cat("\n\n")
    cat("## Average Marginal Component Effects (AMCEs) - Point \n\n")
    print( conjoint_plot_point(amces) + ggplot2::labs(title = dim, subtitle = "Average Marginal Component Effects (AMCEs)") )
    cat("\n\n")
    cat("## Marginal Means\n\n")
    print(conjoint_plot_point(margins) + ggplot2::labs(title = dim, subtitle = "Marginal Means"))
    cat("\n\n")
    cat("## Importance Weights\n\n")
    print(conjoint_plot_point(importance) + 
            ggplot2::scale_x_continuous(labels = scales::label_percent()) + 
            ggplot2::labs(title = dim, subtitle = "Importance Weights")  )
    cat("\n\n")
  }
```

```{r example-conjoint_walk}
kobodata <-  system.file("data-demo/conjoint_data.xlsx", package = "VulnerabilityScoreCalibration")
koboform <-  system.file("data-demo/conjoint_form.xlsx", package = "VulnerabilityScoreCalibration") 

cj <- conjoint_review(kobodata, koboform) 
cjdata <- cj[["cjdata"]]
## Get a summary of all dimensions
purrr::pwalk(cjdata, conjoint_walk)

## Save a csv extract of the weights
# purrr::walk2(cjdata$dim, cjdata$amces, ~write_csv(.y, fs::path(.x, ext = "csv")))

#all <- purrr::walk2(cjdata$amces,  ~cbind())

all <-purrr::pwalk(cjdata$amces, rbind)
all2 <- dplyr::bind_rows(cjdata$amces, .id = "column_label")



```

```{r tests-conjoint_walk}
test_that("conjoint_walk works", {
  expect_true(inherits(conjoint_walk, "function")) 
})
```




<!--
 below is a default chunk to create and document your run_app function
-->

# run_app


<!--
Create a chunk for the core of the function

- The chunk needs to be named `function` at least
- It contains the code of a documented function
- The chunk can also be named `function-my_function` to make it easily
findable in your Rmd
- Let the `@examples` part empty, and use the next `examples` chunk instead to present reproducible examples

After inflating the template

-  This function code will automatically be added in a new file in the "R/" directory
-->
    
```{r function-run_app}
#' Run the Shiny Application
#' 
#' 
#' @param onStart A function that will be called before the app is actually run.
#' This is only needed for \code{shinyAppObj}, since in the \code{shinyAppDir}
#' case, a \code{global.R} file can be used for this purpose.

#' @param options Named options that should be passed to the \code{runApp} call
#' (these can be any of the following: "port", "launch.browser", "host", "quiet",
#' "display.mode" and "test.mode"). You can also specify \code{width} and
#' \code{height} parameters which provide a hint to the embedding environment
#' about the ideal height/width for the app.

#' @param enableBookmarking Can be one of \code{"url"}, \code{"server"}, or
#' \code{"disable"}. The default value, \code{NULL}, will respect the setting from
#' any previous calls to  \code{\link[shiny:enableBookmarking]{enableBookmarking()}}. See 
#' #' \code{\link[shiny:enableBookmarking]{enableBookmarking()}}
#' for more information on bookmarking your app.

#' @param uiPattern A regular expression that will be applied to each \code{GET}
#' request to determine whether the \code{ui} should be used to handle the
#' request. Note that the entire request path must match the regular
#' expression in order for the match to be considered successful.
#' 
#' 
#' @param ... arguments to pass to golem_opts.
#' See `?golem::get_golem_options` for more details.
#'
#' @importFrom shiny shinyApp
#' @importFrom golem with_golem_options
#' 
#' @return a shiny app
#' 
#' @export

run_app <- function(
    onStart = NULL,
    options = list(),
    enableBookmarking = NULL,
    uiPattern = "/",
    ...
) {
  with_golem_options(
    app = shinyApp(
      ui = app_ui,
      server = app_server,
      onStart = onStart,
      options = options,
      enableBookmarking = enableBookmarking,
      uiPattern = uiPattern
    ),
    golem_opts = list(...)
  )
}

```

<!--
Create a chunk with an example of use for your function

- The chunk needs to be named `examples` at least
- It contains working examples of your function
- The chunk is better be named `examples-my_median` to be handled
correctly when inflated as a vignette

After inflating the template

-  This example will automatically be added in the '@examples' part of our function above in the "R/" directory
- This example will automatically be added in the vignette created from this Rmd template
-->

```{r example-run_app}
# run_app()
```

<!--
Create a chunk with a test of use for your function

- The chunk needs to be named `tests` at least
- It contains working tests of your function
- The chunk is better be named `tests-my_median` to be handled
correctly when inflated as a vignette

After inflating the template

-  This test code will automatically be added in the "tests/testthat/" directory
-->
 
```{r tests-run_app}
# test_that("run_app works", {
#   expect_true(inherits(run_app, "function")) 
# })
```
  



<!--
 Once you have created your back office functions , run the next chunk to install and package them

# There can be development actions

Create a chunk with 'development' actions

- The chunk needs to be named `development` or `dev`
- It contains functions that are used for package development only
- Note that you may want to store most of these functions in the 0-dev_history.Rmd file

These are only included in the present flat template file, their content will not be part of the package anywhere else.
-->

```{r development-inflate, eval=FALSE}
# Run but keep eval=FALSE to avoid infinite loop
# Execute in the console directly 
fusen::inflate(flat_file = "dev/function_documentation.Rmd",
               check = TRUE,
               document = TRUE,
               overwrite = TRUE, 
               vignette_name = "D-Package Functions with Examples")
```

