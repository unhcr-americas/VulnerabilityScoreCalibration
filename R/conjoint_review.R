# WARNING - Generated by {fusen} from /dev/package_functions.Rmd: do not edit by hand

#' conjoint_review
#' 
#'  What is Conjoint analysis?
#'  
#'  Conjoint analysis can speed up expert consultations by offering an 
#'  __objective mean to compile expert opinions__.
#'  
#'  * Conjoint analysis originated in mathematical psychology by psychometricians.
#'  
#'  * often used to evaluate how people make decisions between a set of 
#'  different options when considering a number of criteria at the same time 
#'  (conjoint features; “trade-offs”). 
#'  
#'  1. Measurement framework
#'  
#'  The [Joint Intersectoral Analysis Framework (JIAF)](https://interagencystandingcommittee.org/system/files/gbna_tools_and_guidance_pkg_final_june_2019.pdf) 
#'  is a theoretical generic measurement framework to be used for Humanitarian 
#'  needs assessment. It specifies three distinct and complementary components
#'   of humanitarian severity and vulnerability indexes:
#'    
#'  * Basic Needs & Living standards  
#'  
#'  * Coping Capacity
#'  
#'  * Well Being & Community integration
#'  
#'  This generic model can be contextualised: different sub-indicators might be 
#'  used for each of the 3 components depending on cultural and political situations. 
#'  
#'  2. Define the combined alternatives to be compared 
#'  
#'  * participants rate their preferences for profiles with different combinations
#'   of the attributes or criteria. 
#'  
#'  * CA then allows to “decompose” or reverse-engineer these ratings into
#'   estimates of how important each criteria or attribute is to a participant’s ranking decisions
#'  
#'  3. Utility scales & Agreement levels
#'  
#'  Estimating the contribution of each potential answers
#'  
#'  * Utility values indicate the overall contribution of each attribute to how 
#'  the profiles were rated (e.g. whether number of meals is more important in 
#'  vulnerability scoring than access to safe water). 
#'  
#'  * A higher _"utility"_ estimate indicates that this level contributes to a
#'   higher vulnerability than the level with the lower utility estimate 
#'   (it does not give an absolute value for the utility of an option, but rather
#'    assumes a reference alternative). 
#'  
#'  * Standard deviation for each level within model allows to better understand 
#'  how homogeneous the group of experts is with respect to one level.  
#'  
#'  4. Importance of each criteria 
#'  
#'   * Importance of each criteria represent the average importance as estimated from all experts.
#'  
#'   * Importance values will then be used as the weights for each attribute inside each of our three dimensions. 
#'  
#'   * Importance values sum to 100%. 
#'   
#' @import ggplot2
#' @importFrom unhcrthemes theme_unhcr
#' @import survival
#' @import sandwich
#' @import conjoint
#' @importFrom readxl read_excel
#' @importFrom stringr str_c str_replace str_match
#' @importFrom purrr compose
#' @importFrom tidyr separate_rows nest pivot_longer  pivot_wider
#' @importFrom cregg cj mm
#' @importFrom stats mahalanobis
#' @importFrom forcats as_factor
#' @importFrom lubridate as.duration
#' @importFrom tidyselect starts_with
#' @importFrom  dplyr select mutate distinct transmute
#'                    with_groups  as_tibble rowwise
#'                   arrange row_number anti_join first
#'                   across everything where summarize
#' 
#' @return a series of plot
#' 
#' @export
#' @examples
#' 
#' kobodata <-  system.file("data-demo/conjoint_data.xlsx", package = "VulnerabilityScoreCalibration")
#' koboform <-  system.file("data-demo/conjoint_form.xlsx", package = "VulnerabilityScoreCalibration") 
#' 
#' cj <- conjoint_review(kobodata, koboform)
#' 
#' cj[["data_quality"]]
conjoint_review <- function(kobodata, koboform){
   ## load data
  data <- readxl::read_excel(kobodata,
                       sheet = 1)
  
  ## load info from the form
  form <- readxl::read_excel(koboform,
                       sheet = "survey") |>
    ## Rename and use what ever label set is coming first 
    dplyr::rename(label = dplyr::first(tidyselect::starts_with("label")),
                  hint = dplyr::first(tidyselect::starts_with("hint")))
  
  opts <- readxl::read_excel(koboform,
                       sheet = "opts")
  
  
   opts <- opts |> 
     dplyr::mutate(dplyr::across(dplyr::everything(), 
                          stringr::str_replace, "\r", ""))
  
  dict <- opts |>
    dplyr::distinct(dim, measure) |>
    dplyr::arrange(dim, measure) |>
    dplyr::with_groups(dim, 
                       mutate, 
                       feature = stringr::str_c("v", 
                                                dplyr::row_number()))
  
  scores <- data |>
    dplyr::select(email, starts_with("p.")) |>
    tidyr::pivot_longer(-email, names_to = "var")
  
  dims <- data |>
    dplyr::select(starts_with("p.")) |>
    names() |>
    stringr::str_match("p\\.(.+)\\.(.+)") |>
    dplyr::as_tibble(.name_repair = "universal") |>
    dplyr::rename(var = ...1, dim = ...2, lvl = ...3) |>
    dplyr::left_join(form |> 
                       dplyr::select(lvl = name, label = `label`), 
                     by = "lvl") |>
    tidyr::separate_rows(label, sep = "\\n") |>
    dplyr::with_groups(lvl, mutate, 
                       feature = stringr::str_c("v", 
                                                dplyr::row_number())) |>
    dplyr::left_join(dict) |> 
    dplyr::select(-feature) |>
    tidyr::pivot_wider(names_from = measure, values_from = label) |>
    dplyr::mutate(dplyr:::across(-c(var:lvl), forcats::as_factor)) 

#uncommented as_factor
#   ~factor(., levels = opts |> filter(measure == cur_column()) |> pull(level)))) 

  cjdata <-  dplyr::left_join(dims, scores, by = "var") |>
    tidyr::nest(data = -dim) |>
    dplyr::rowwise() |>
    dplyr::mutate(
      data = data |> 
        select(- purrr::compose( purrr::compose(all, is.na))) |> 
        list(),
      formula =
        data |>
        dplyr::select(where(is.factor)) |>
        names() |>
        stringr::str_c(collapse = "+") |>
        stringr::str_c("value", 
                       vars = _, 
                       sep = "~"),
        margins = cregg::mm(data, as.formula(formula), id = ~ email) |> 
        list(),
      amces =
        cregg::cj(data, 
                  as.formula(formula),
                  id = ~ email) |>
        dplyr::group_by(feature) |>
        dplyr::mutate(
          normalized = (estimate - min(estimate)) / (max(estimate) - min(estimate)),
          horizontal = (estimate - min(estimate)),
          normalized1p = normalized + 1,
          horizontal1p = horizontal + 1 ) |> 
        dplyr::ungroup() |> 
        list(),
      importance =
        dplyr::as_tibble(amces) |>
        dplyr::mutate(
          estimate = abs(estimate) / sum(abs(estimate)),
          lower = abs(lower) / sum(abs(lower), na.rm = TRUE),
          upper = abs(upper) / sum(abs(upper), na.rm = TRUE)  ) |> 
        list()
    )


  ##This is where cleaning is reported, identifying cases for deletion. 
  
  #FOR TIME: It is recommended that experts complete the survey during the live exercise. 
  # A good window of time is between 10-90 minutes. Cases that were completed faster
  #or slower than this should be considered for deletion. 
  #This code generated the table of flagged entries. click on 'expert_validation'
  #on the Environment to see the table. If you identify any entry flagged as suspect,
  #but that you still want to keep, change the reference values 10 or 90. 
  time_validation <-  data |>
    dplyr::transmute(
      email,
      dur = lubridate::as.duration(end - start) |> 
        as.numeric("minutes"),
      too_short = dur < 10,
      too_long = dur > 90,
      suspect = too_short | too_long
    )
  # View(time_validation)
  data <- data |> 
      dplyr::anti_join(time_validation |> 
                         dplyr::filter(suspect), 
                       by = "email")

  #FOR DUPLICATE ENTIRES: Cleaning for duplicates based on name of expert.
  
  #this identifies duplicates by name. Look and manually choose which to keep.
  # expertdups <-   data |> 
  #   dplyr::semi_join(count(., email) |> 
  #                      dplyr::filter(n > 1)) 
  #Example manual cleaning.
  # data <-
  #   data[!(data$"_id" %in% c("26964652")),] 
  
  # 1 duplicate entries. Kept shortest time. Eliminated _id is 26964652. 
  # Kept entry's _id is 26964599


  #FOR OUTLIERS: Screen for data quality.
  #Spot repetitive responses (example: answered all options with '3'), responses 
  # with little variance (all answers between 6-7, standard deviation less than 2). 
  # Also looks for averages that are very different from the rest of the responses.
  # For example, someone's average score is 2 when the rest of the scores are 
  # between 6-7. Analysis can be repeated with and without this case to test and 
  # compare how it influences the results. 
   
  
  #spotting outliers using Mahalanobis distance. 
  mah.data <-   data |> 
    dplyr::select(starts_with("p.")) |> 
    dplyr::select(where(is.numeric))


  #just added this ", tol=1e-21" right after cov(.). After error message:
  #"system is computationally singular: reciprocal condition number = 4.08252e-20 
  #Input `mah.dist` is `mah.data |> mahalanobis(., center = colMeans(.), cov = cov(.))`.

  # expert <-  data |> 
  #   mutate(mah.dist = mah.data |> 
  #            stats::mahalanobis( .,  
  #                                center = colMeans(.), 
  #                                cov = cov(.), 
  #                                tol = 1e-21 )) 

#Cutoff point of probability, can be modified. .99 extreme outliers, .95, .90, etc. 
#data <- data |> anti_join(outliers, by = "_id") 

#filters the identified out.
#PANAMA: V1 of the analysis - 4 outliers identified at .95, .97 and .99 levels (same 4 outliers). But they belonged to experts considered of good opinion. Analysis was run first including all outliers. In this analysis, for "Coping Capacities, there were higher levels of discrepancy. Where the dot was outside of the lines. There was also no differences between the levels of "type of household head". 
#PANAMA: analysis was run a second time removing all 4 outliers. The results were the same. Outliers were mantained. 
#PANAMA: V2 of the analysis - 2 outliers were detected at .95, .97 and 1 detected at the .99 level (the 1 outlier was included in all probability levels. It was decided to keep the outlier. 

  # outliers <- expert |> 
  #   dplyr::filter(mah.dist > qchisq(.99, 
  #                                   df = ncol(mah.data))) 
  # 
  # nexpert <- n_distinct(data$email)
  #names(expert)

  ## Data Quality Viz
  plot1 <- data |>
    ggplot(aes(as.numeric(survey_mins))) +
    geom_histogram(color = "white", 
                   fill = "#0072BC") +
    scale_y_continuous(expand = expansion(0, 0)) +
    labs(title = "Times to complete the consultation",
         x = "minutes", y = "# of participants") +
    unhcrthemes::theme_unhcr()
  
  plot2 <- data |>
    dplyr::summarize(mean = rowMeans(data |> 
                                       dplyr::select(starts_with("p.")))) |>
    ggplot(aes(mean)) +
    geom_histogram(colour = "white",
                   fill = "#0072BC",
                   binwidth = 1) +
    scale_y_continuous(expand = expansion(0, 0)) +
    labs(title = "Average score",
         x = "", y = "# de participantes") +
    unhcrthemes::theme_unhcr()

  require(patchwork)
  p <- plot1 + plot2 + plot_annotation(caption = glue::glue("Based on {nrow(data)} consultations"))

  results <- list(
    data =  data,
    cjdata = cjdata,
    data_quality = p
  )
   return(results)
}
